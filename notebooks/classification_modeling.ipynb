{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelado de Clasificación - CS:GO Dataset\n",
        "\n",
        "**Objetivo:** Entrenar modelos de clasificación para predecir supervivencia del jugador en CS:GO\n",
        "\n",
        "**Target Principal:** Survived (supervivencia del jugador en la ronda)\n",
        "\n",
        "**Metodología:** Comparación de 5 algoritmos con GridSearchCV y análisis de curvas ROC\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importación de Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Librerías principales\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Scikit-learn para ML\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, \n",
        "    confusion_matrix, classification_report, roc_auc_score, \n",
        "    roc_curve, precision_recall_curve, auc\n",
        ")\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Configuración de visualización\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"Librerías importadas exitosamente\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"Numpy version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Carga y Exploración de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar dataset con features\n",
        "try:\n",
        "    df = pd.read_csv(\"../data/04_feature/csgo_data_with_features.csv\")\n",
        "    print(f\"Dataset con features cargado: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Archivo con features no encontrado. Cargando datos limpios...\")\n",
        "    df = pd.read_csv(\"../data/02_intermediate/csgo_data_clean.csv\")\n",
        "    print(f\"Dataset limpio cargado: {df.shape}\")\n",
        "\n",
        "# Información básica\n",
        "print(f\"\\nInformación del dataset:\")\n",
        "print(f\"- Filas: {df.shape[0]:,}\")\n",
        "print(f\"- Columnas: {df.shape[1]}\")\n",
        "print(f\"- Memoria: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Verificar que tenemos la variable target\n",
        "target = 'Survived'\n",
        "if target not in df.columns:\n",
        "    print(f\"\\nADVERTENCIA: Columna {target} no encontrada\")\n",
        "    print(f\"Columnas disponibles: {list(df.columns)}\")\nelse:\n",
        "    print(f\"\\nTarget variable encontrada: {target}\")\n",
        "\n",
        "# Mostrar primeras filas\n",
        "print(f\"\\nPrimeras 5 filas:\")\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Análisis del Target y Balanceamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analizar target variable\n",
        "print(f\"ANÁLISIS DEL TARGET: {target}\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Convertir a numérico si es necesario\n",
        "if df[target].dtype == 'object' or df[target].dtype == 'bool':\n",
        "    le = LabelEncoder()\n",
        "    df[target + '_encoded'] = le.fit_transform(df[target])\n",
        "    target_encoded = target + '_encoded'\n",
        "    \n",
        "    print(f\"Target original: {df[target].value_counts().to_dict()}\")\n",
        "    print(f\"Target codificado: {df[target_encoded].value_counts().to_dict()}\")\n",
        "    print(f\"Mapeo: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
        "    target_col = target_encoded\nelse:\n",
        "    target_col = target\n",
        "\n",
        "# Distribución del target\n",
        "target_counts = df[target_col].value_counts()\n",
        "target_props = df[target_col].value_counts(normalize=True)\n",
        "\n",
        "print(f\"\\nDistribución del target:\")\n",
        "for val, count in target_counts.items():\n",
        "    prop = target_props[val]\n",
        "    print(f\"Clase {val}: {count:,} ({prop:.3f})\")\n",
        "\n",
        "# Verificar balanceamiento\n",
        "minority_prop = min(target_props)\n",
        "majority_prop = max(target_props)\n",
        "balance_ratio = minority_prop / majority_prop\n",
        "\n",
        "print(f\"\\nAnálisis de balanceamiento:\")\n",
        "print(f\"Ratio minority/majority: {balance_ratio:.3f}\")\n",
        "\n",
        "if balance_ratio >= 0.8:\n",
        "    balance_status = \"BIEN BALANCEADO\"\nelif balance_ratio >= 0.6:\n",
        "    balance_status = \"MODERADAMENTE BALANCEADO\"\nelse:\n",
        "    balance_status = \"DESBALANCEADO\"\n",
        "\n",
        "print(f\"Estado: {balance_status}\")\n",
        "\n",
        "# Visualización del target\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gráfico de barras\n",
        "target_counts.plot(kind='bar', ax=axes[0], color=['lightcoral', 'lightblue'], alpha=0.8)\n",
        "axes[0].set_title(f'Distribución de {target}', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Clase')\n",
        "axes[0].set_ylabel('Frecuencia')\n",
        "axes[0].tick_params(axis='x', rotation=0)\n",
        "\n",
        "# Añadir etiquetas a las barras\n",
        "for i, v in enumerate(target_counts.values):\n",
        "    axes[0].text(i, v + len(df)*0.01, f'{v:,}\\n({target_props.iloc[i]:.1%})', \n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Gráfico circular\n",
        "axes[1].pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%', \n",
        "           colors=['lightcoral', 'lightblue'], startangle=90)\n",
        "axes[1].set_title(f'Proporción de {target}', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Selección de Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seleccionar features numéricas (excluyendo target y variables problemáticas)\n",
        "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Excluir variables que causan data leakage o son irrelevantes\n",
        "exclude_features = [\n",
        "    target, target_col,  # Target variables\n",
        "    'RoundWinner', 'MatchWinner',  # Resultado de la ronda/match (data leakage)\n",
        "    'MatchKills', 'MatchAssists', 'MatchHeadshots', 'MatchFlankKills',  # Métricas de match completo\n",
        "    'Total_Match_Actions', 'Match_Kill_Per_Round', 'Match_Assist_Per_Round',  # Derivadas de match\n",
        "    'Cumulative_Kills',  # Acumulativa de kills\n",
        "    'Team_Win_Rate',  # Podría causar leakage temporal\n",
        "    'Unnamed: 0', 'MatchId', 'InternalTeamId', 'Player_ID'  # IDs\n",
        "]\n",
        "\n",
        "# Filtrar features candidatas\n",
        "candidate_features = [f for f in numeric_features if f not in exclude_features]\n",
        "\n",
        "print(f\"Features candidatas: {len(candidate_features)}\")\n",
        "print(f\"Primeras 10 features: {candidate_features[:10]}\")\n",
        "\n",
        "# Calcular correlación con el target\n",
        "correlations = df[candidate_features + [target_col]].corr()[target_col].abs().sort_values(ascending=False)\n",
        "correlations = correlations.drop(target_col)  # Remover autocorrelación\n",
        "\n",
        "print(f\"\\nTop 15 correlaciones con {target}:\")\n",
        "for i, (feature, corr) in enumerate(correlations.head(15).items()):\n",
        "    direction = \"(+)\" if df[feature].corr(df[target_col]) > 0 else \"(-)\"\n",
        "    print(f\"{i+1:2d}. {feature:30s}: {corr:.3f} {direction}\")\n",
        "\n",
        "# Visualizar top correlaciones\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_corr = correlations.head(15)\n",
        "colors = ['green' if df[feature].corr(df[target_col]) > 0 else 'red' for feature in top_corr.index]\n",
        "\n",
        "plt.barh(range(len(top_corr)), top_corr.values, color=colors, alpha=0.7)\n",
        "plt.yticks(range(len(top_corr)), top_corr.index)\n",
        "plt.xlabel('Correlación Absoluta')\n",
        "plt.title(f'Top 15 Correlaciones con {target}', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Añadir valores a las barras\n",
        "for i, v in enumerate(top_corr.values):\n",
        "    plt.text(v + 0.005, i, f'{v:.3f}', va='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Selección final de features evitando multicolinealidad\n",
        "selected_features = [\n",
        "    'RoundKills',                    # Más kills = mayor probabilidad supervivencia\n",
        "    'RoundAssists',                  # Asistencias indican participación\n",
        "    'Total_Combat_Actions',          # Actividad general en combate\n",
        "    'RoundStartingEquipmentValue',   # Mejor equipamiento = mayor supervivencia\n",
        "    'Equipment_ROI',                 # Eficiencia del equipamiento\n",
        "    'Team_Coordination_Score',       # Coordinación del equipo\n",
        "    'RLethalGrenadesThrown',         # Uso táctico de granadas\n",
        "    'RNonLethalGrenadesThrown',      # Utilidades para supervivencia\n",
        "    'Individual_Equipment_Share',    # Proporción de equipamiento del jugador\n",
        "    'Kill_Equipment_Efficiency',     # Eficiencia individual\n",
        "    'RoundId',                       # Progreso de la partida\n",
        "    'Equipment_Kills_Interaction',   # Interacción equipamiento-rendimiento\n",
        "    'Above_Team_Average_Equipment',  # Ventaja relativa en equipamiento\n",
        "    'Team_Total_Kills',             # Rendimiento del equipo\n",
        "    'Round_Equipment_Interaction'    # Interacción ronda-equipamiento\n",
        "]\n",
        "\n",
        "# Verificar que las features existen\n",
        "available_features = [f for f in selected_features if f in df.columns]\n",
        "missing_features = [f for f in selected_features if f not in df.columns]\n",
        "\n",
        "if missing_features:\n",
        "    print(f\"Features faltantes: {missing_features}\")\n",
        "    selected_features = available_features\n",
        "\n",
        "print(f\"\\nFEATURES SELECCIONADAS PARA CLASIFICACIÓN: {len(selected_features)}\")\n",
        "for i, feature in enumerate(selected_features):\n",
        "    if feature in df.columns:\n",
        "        corr_with_target = df[feature].corr(df[target_col])\n",
        "        print(f\"{i+1:2d}. {feature:30s}: r = {corr_with_target:.3f}\")\n",
        "\n",
        "# Verificar valores nulos\n",
        "null_counts = df[selected_features + [target_col]].isnull().sum()\n",
        "if null_counts.sum() > 0:\n",
        "    print(f\"\\nValores nulos encontrados:\")\n",
        "    print(null_counts[null_counts > 0])\n",
        "    \n",
        "    # Imputar valores nulos si es necesario\n",
        "    for col in selected_features:\n",
        "        if df[col].isnull().sum() > 0:\n",
        "            df[col].fillna(df[col].median(), inplace=True)\n",
        "            print(f\"Imputados valores nulos en {col} con la mediana\")\nelse:\n",
        "    print(f\"\\nNo hay valores nulos en las features seleccionadas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Preparación de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar datos para modelado\n",
        "X = df[selected_features].copy()\n",
        "y = df[target_col].copy()\n",
        "\n",
        "print(f\"Shape de X: {X.shape}\")\n",
        "print(f\"Shape de y: {y.shape}\")\n",
        "print(f\"Distribución de y: {y.value_counts().to_dict()}\")\n",
        "\n",
        "# Split estratificado train/test (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nDivisión train/test:\")\n",
        "print(f\"X_train: {X_train.shape}\")\n",
        "print(f\"X_test: {X_test.shape}\")\n",
        "print(f\"y_train distribución: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"y_test distribución: {y_test.value_counts().to_dict()}\")\n",
        "\n",
        "# Escalado de features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convertir de vuelta a DataFrame\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=selected_features, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=selected_features, index=X_test.index)\n",
        "\n",
        "print(f\"\\nDatos escalados exitosamente\")\n",
        "print(f\"Media de X_train_scaled: {X_train_scaled.mean().mean():.6f}\")\n",
        "print(f\"Std de X_train_scaled: {X_train_scaled.std().mean():.6f}\")\n",
        "\n",
        "# Visualizar distribución de algunas features clave\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "key_features = ['RoundKills', 'RoundStartingEquipmentValue', 'Equipment_ROI', \n",
        "                'Total_Combat_Actions', 'Team_Coordination_Score', 'RoundId']\n",
        "\n",
        "for i, feature in enumerate(key_features):\n",
        "    if feature in selected_features:\n",
        "        # Separar por clase\n",
        "        class_0 = X_train[y_train == 0][feature]\n",
        "        class_1 = X_train[y_train == 1][feature]\n",
        "        \n",
        "        axes[i].hist(class_0, bins=30, alpha=0.7, label='No Sobrevivió', color='red')\n",
        "        axes[i].hist(class_1, bins=30, alpha=0.7, label='Sobrevivió', color='blue')\n",
        "        axes[i].set_title(f'Distribución de {feature}', fontweight='bold')\n",
        "        axes[i].set_xlabel(feature)\n",
        "        axes[i].set_ylabel('Frecuencia')\n",
        "        axes[i].legend()\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Definición de Modelos y Parámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir modelos y sus parámetros para GridSearch\n",
        "models = {\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [10, 15, 20, None],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4]\n",
        "        }\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'model': GradientBoostingClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'learning_rate': [0.05, 0.1, 0.15],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'subsample': [0.8, 0.9, 1.0]\n",
        "        }\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'model': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'learning_rate': [0.05, 0.1, 0.15],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'subsample': [0.8, 0.9, 1.0]\n",
        "        }\n",
        "    },\n",
        "    'Logistic Regression': {\n",
        "        'model': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
        "        'params': {\n",
        "            'C': [0.1, 1.0, 10.0, 100.0],\n",
        "            'solver': ['liblinear', 'lbfgs'],\n",
        "            'penalty': ['l1', 'l2']\n",
        "        }\n",
        "    },\n",
        "    'Support Vector Classifier': {\n",
        "        'model': SVC(random_state=42, class_weight='balanced', probability=True),\n",
        "        'params': {\n",
        "            'C': [0.1, 1, 10, 100],\n",
        "            'gamma': ['scale', 'auto', 0.001, 0.01],\n",
        "            'kernel': ['rbf', 'linear']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"MODELOS DEFINIDOS: {len(models)}\")\n",
        "for name, config in models.items():\n",
        "    param_combinations = 1\n",
        "    for param, values in config['params'].items():\n",
        "        param_combinations *= len(values)\n",
        "    print(f\"- {name}: {param_combinations} combinaciones de parámetros\")\n",
        "\n",
        "print(f\"\\nConfiguración de GridSearchCV:\")\n",
        "print(f\"- Validación cruzada: 5-fold estratificada\")\n",
        "print(f\"- Métrica de scoring: ROC AUC\")\n",
        "print(f\"- Class weight: balanced para modelos que lo soportan\")\n",
        "print(f\"- Paralelización: todos los núcleos disponibles\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Entrenamiento de Modelos con GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diccionario para almacenar resultados\n",
        "results = {}\n",
        "best_models = {}\n",
        "\n",
        "print(\"INICIANDO ENTRENAMIENTO CON GRIDSEARCHCV\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Configurar validación cruzada estratificada\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for name, config in models.items():\n",
        "    print(f\"\\nEntrenando {name}...\")\n",
        "    \n",
        "    # Ajustar parámetros para Logistic Regression\n",
        "    if name == 'Logistic Regression':\n",
        "        # Filtrar combinaciones inválidas (l1 no funciona con lbfgs)\n",
        "        param_grid = []\n",
        "        for C in config['params']['C']:\n",
        "            for solver in config['params']['solver']:\n",
        "                for penalty in config['params']['penalty']:\n",
        "                    if not (solver == 'lbfgs' and penalty == 'l1'):\n",
        "                        param_grid.append({'C': C, 'solver': solver, 'penalty': penalty})\n",
        "        \n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=config['model'],\n",
        "            param_grid=param_grid,\n",
        "            cv=cv_strategy,\n",
        "            scoring='roc_auc',\n",
        "            n_jobs=-1,\n",
        "            verbose=1\n",
        "        )\n",
        "    else:\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=config['model'],\n",
        "            param_grid=config['params'],\n",
        "            cv=cv_strategy,\n",
        "            scoring='roc_auc',\n",
        "            n_jobs=-1,\n",
        "            verbose=1\n",
        "        )\n",
        "    \n",
        "    # Entrenar\n",
        "    if name in ['Logistic Regression', 'Support Vector Classifier']:\n",
        "        # Usar datos escalados para modelos sensibles a escala\n",
        "        grid_search.fit(X_train_scaled, y_train)\n",
        "        X_train_used = X_train_scaled\n",
        "        X_test_used = X_test_scaled\n",
        "    else:\n",
        "        # Usar datos originales para modelos basados en árboles\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        X_train_used = X_train\n",
        "        X_test_used = X_test\n",
        "    \n",
        "    # Mejores parámetros\n",
        "    best_params = grid_search.best_params_\n",
        "    best_score = grid_search.best_score_\n",
        "    \n",
        "    # Predicciones\n",
        "    y_train_pred = grid_search.predict(X_train_used)\n",
        "    y_test_pred = grid_search.predict(X_test_used)\n",
        "    y_train_proba = grid_search.predict_proba(X_train_used)[:, 1]\n",
        "    y_test_proba = grid_search.predict_proba(X_test_used)[:, 1]\n",
        "    \n",
        "    # Métricas\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    train_precision = precision_score(y_train, y_train_pred)\n",
        "    test_precision = precision_score(y_test, y_test_pred)\n",
        "    train_recall = recall_score(y_train, y_train_pred)\n",
        "    test_recall = recall_score(y_test, y_test_pred)\n",
        "    train_f1 = f1_score(y_train, y_train_pred)\n",
        "    test_f1 = f1_score(y_test, y_test_pred)\n",
        "    train_auc = roc_auc_score(y_train, y_train_proba)\n",
        "    test_auc = roc_auc_score(y_test, y_test_proba)\n",
        "    \n",
        "    # Almacenar resultados\n",
        "    results[name] = {\n",
        "        'best_params': best_params,\n",
        "        'cv_auc': best_score,\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'train_precision': train_precision,\n",
        "        'test_precision': test_precision,\n",
        "        'train_recall': train_recall,\n",
        "        'test_recall': test_recall,\n",
        "        'train_f1': train_f1,\n",
        "        'test_f1': test_f1,\n",
        "        'train_auc': train_auc,\n",
        "        'test_auc': test_auc,\n",
        "        'y_train_pred': y_train_pred,\n",
        "        'y_test_pred': y_test_pred,\n",
        "        'y_train_proba': y_train_proba,\n",
        "        'y_test_proba': y_test_proba\n",
        "    }\n",
        "    \n",
        "    best_models[name] = grid_search.best_estimator_\n",
        "    \n",
        "    print(f\"  CV AUC: {best_score:.4f}\")\n",
        "    print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"  Test AUC: {test_auc:.4f}\")\n",
        "    print(f\"  Test F1: {test_f1:.4f}\")\n",
        "    print(f\"  Mejores parámetros: {best_params}\")\n",
        "\n",
        "print(f\"\\nENTRENAMIENTO COMPLETADO\")\n",
        "print(\"=\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Comparación de Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear DataFrame con resultados\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': list(results.keys()),\n",
        "    'CV_AUC': [results[model]['cv_auc'] for model in results.keys()],\n",
        "    'Train_Accuracy': [results[model]['train_accuracy'] for model in results.keys()],\n",
        "    'Test_Accuracy': [results[model]['test_accuracy'] for model in results.keys()],\n",
        "    'Train_Precision': [results[model]['train_precision'] for model in results.keys()],\n",
        "    'Test_Precision': [results[model]['test_precision'] for model in results.keys()],\n",
        "    'Train_Recall': [results[model]['train_recall'] for model in results.keys()],\n",
        "    'Test_Recall': [results[model]['test_recall'] for model in results.keys()],\n",
        "    'Train_F1': [results[model]['train_f1'] for model in results.keys()],\n",
        "    'Test_F1': [results[model]['test_f1'] for model in results.keys()],\n",
        "    'Train_AUC': [results[model]['train_auc'] for model in results.keys()],\n",
        "    'Test_AUC': [results[model]['test_auc'] for model in results.keys()]\n",
        "})\n",
        "\n",
        "# Ordenar por Test AUC\n",
        "results_df = results_df.sort_values('Test_AUC', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"COMPARACIÓN DE RESULTADOS\")\n",
        "print(\"=\" * 40)\n",
        "display(results_df.round(4))\n",
        "\n",
        "# Identificar mejor modelo\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "best_test_auc = results_df.iloc[0]['Test_AUC']\n",
        "\n",
        "print(f\"\\nMEJOR MODELO: {best_model_name}\")\n",
        "print(f\"Test AUC: {best_test_auc:.4f}\")\n",
        "print(f\"Test Accuracy: {results_df.iloc[0]['Test_Accuracy']:.4f}\")\n",
        "print(f\"Test F1: {results_df.iloc[0]['Test_F1']:.4f}\")\n",
        "\n",
        "# Verificar overfitting/underfitting\n",
        "print(f\"\\nANÁLISIS DE OVERFITTING/UNDERFITTING:\")\n",
        "for idx, row in results_df.iterrows():\n",
        "    model_name = row['Model']\n",
        "    train_auc = row['Train_AUC']\n",
        "    test_auc = row['Test_AUC']\n",
        "    diff = train_auc - test_auc\n",
        "    \n",
        "    if diff > 0.1:\n",
        "        status = \"OVERFITTING\"\n",
        "    elif test_auc < 0.7:\n",
        "        status = \"UNDERFITTING\"\n",
        "    else:\n",
        "        status = \"BUENO\"\n",
        "    \n",
        "    print(f\"{model_name:25s}: Train={train_auc:.3f}, Test={test_auc:.3f}, Diff={diff:.3f} [{status}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualización de resultados\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Comparación de AUC scores\n",
        "x_pos = np.arange(len(results_df))\n",
        "width = 0.35\n",
        "\n",
        "axes[0,0].bar(x_pos - width/2, results_df['Train_AUC'], width, label='Train AUC', alpha=0.8, color='skyblue')\n",
        "axes[0,0].bar(x_pos + width/2, results_df['Test_AUC'], width, label='Test AUC', alpha=0.8, color='orange')\n",
        "axes[0,0].set_xlabel('Modelos')\n",
        "axes[0,0].set_ylabel('AUC Score')\n",
        "axes[0,0].set_title('Comparación AUC Score: Train vs Test', fontweight='bold')\n",
        "axes[0,0].set_xticks(x_pos)\n",
        "axes[0,0].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "axes[0,0].axhline(y=0.8, color='green', linestyle='--', alpha=0.7, label='Objetivo AUC = 0.8')\n",
        "\n",
        "# 2. F1 Score Comparison\n",
        "axes[0,1].bar(x_pos - width/2, results_df['Train_F1'], width, label='Train F1', alpha=0.8, color='lightgreen')\n",
        "axes[0,1].bar(x_pos + width/2, results_df['Test_F1'], width, label='Test F1', alpha=0.8, color='lightcoral')\n",
        "axes[0,1].set_xlabel('Modelos')\n",
        "axes[0,1].set_ylabel('F1 Score')\n",
        "axes[0,1].set_title('Comparación F1 Score: Train vs Test', fontweight='bold')\n",
        "axes[0,1].set_xticks(x_pos)\n",
        "axes[0,1].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Test AUC ranking\n",
        "axes[1,0].barh(range(len(results_df)), results_df['Test_AUC'], color='purple', alpha=0.7)\n",
        "axes[1,0].set_yticks(range(len(results_df)))\n",
        "axes[1,0].set_yticklabels(results_df['Model'])\n",
        "axes[1,0].set_xlabel('Test AUC Score')\n",
        "axes[1,0].set_title('Ranking de Modelos por Test AUC', fontweight='bold')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "axes[1,0].axvline(x=0.8, color='green', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Añadir valores a las barras\n",
        "for i, v in enumerate(results_df['Test_AUC']):\n",
        "    axes[1,0].text(v + 0.01, i, f'{v:.3f}', va='center', fontweight='bold')\n",
        "\n",
        "# 4. Métricas múltiples del mejor modelo\n",
        "best_metrics = {\n",
        "    'Accuracy': results_df.iloc[0]['Test_Accuracy'],\n",
        "    'Precision': results_df.iloc[0]['Test_Precision'],\n",
        "    'Recall': results_df.iloc[0]['Test_Recall'],\n",
        "    'F1-Score': results_df.iloc[0]['Test_F1'],\n",
        "    'AUC': results_df.iloc[0]['Test_AUC']\n",
        "}\n",
        "\n",
        "metric_names = list(best_metrics.keys())\n",
        "metric_values = list(best_metrics.values())\n",
        "\n",
        "axes[1,1].bar(metric_names, metric_values, color=['gold', 'lightblue', 'lightgreen', 'orange', 'purple'], alpha=0.8)\n",
        "axes[1,1].set_ylabel('Score')\n",
        "axes[1,1].set_title(f'Métricas del Mejor Modelo: {best_model_name}', fontweight='bold')\n",
        "axes[1,1].set_ylim(0, 1)\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Añadir valores a las barras\n",
        "for i, v in enumerate(metric_values):\n",
        "    axes[1,1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Análisis de Curvas ROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generar curvas ROC para todos los modelos\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
        "\n",
        "for i, (model_name, model_results) in enumerate(results.items()):\n",
        "    # Calcular curva ROC\n",
        "    fpr, tpr, _ = roc_curve(y_test, model_results['y_test_proba'])\n",
        "    auc_score = model_results['test_auc']\n",
        "    \n",
        "    # Plotear curva ROC\n",
        "    plt.plot(fpr, tpr, color=colors[i], lw=2, \n",
        "             label=f'{model_name} (AUC = {auc_score:.3f})')\n",
        "\n",
        "# Línea diagonal (clasificador aleatorio)\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', alpha=0.7, label='Random Classifier')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
        "plt.ylabel('True Positive Rate (Sensitivity)', fontsize=12)\n",
        "plt.title('Curvas ROC - Comparación de Modelos', fontsize=16, fontweight='bold')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Añadir línea de referencia para AUC = 0.8\n",
        "plt.axhline(y=0.8, color='green', linestyle=':', alpha=0.7, label='Objetivo TPR = 0.8')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Tabla resumen de AUC\n",
        "auc_summary = pd.DataFrame({\n",
        "    'Model': list(results.keys()),\n",
        "    'AUC_Score': [results[model]['test_auc'] for model in results.keys()],\n",
        "    'Status': ['EXCELENTE' if results[model]['test_auc'] >= 0.9 else \n",
        "               'BUENO' if results[model]['test_auc'] >= 0.8 else \n",
        "               'ACEPTABLE' if results[model]['test_auc'] >= 0.7 else 'POBRE' \n",
        "               for model in results.keys()]\n",
        "}).sort_values('AUC_Score', ascending=False)\n",
        "\n",
        "print(\"\\nRESUMEN DE AUC SCORES:\")\n",
        "print(\"=\" * 35)\n",
        "display(auc_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Matrices de Confusión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matrices de confusión para todos los modelos\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, (model_name, model_results) in enumerate(results.items()):\n",
        "    # Calcular matriz de confusión\n",
        "    cm = confusion_matrix(y_test, model_results['y_test_pred'])\n",
        "    \n",
        "    # Normalizar para mostrar porcentajes\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    \n",
        "    # Crear heatmap\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', \n",
        "                xticklabels=['No Sobrevivió', 'Sobrevivió'],\n",
        "                yticklabels=['No Sobrevivió', 'Sobrevivió'],\n",
        "                ax=axes[i])\n",
        "    \n",
        "    axes[i].set_title(f'{model_name}\\nAccuracy: {model_results[\"test_accuracy\"]:.3f}', \n",
        "                     fontweight='bold')\n",
        "    axes[i].set_xlabel('Predicción')\n",
        "    axes[i].set_ylabel('Valor Real')\n",
        "    \n",
        "    # Añadir números absolutos\n",
        "    for j in range(2):\n",
        "        for k in range(2):\n",
        "            axes[i].text(k+0.5, j+0.7, f'n={cm[j,k]}', \n",
        "                        ha='center', va='center', fontsize=10, color='red')\n",
        "\n",
        "# Ocultar el subplot extra\n",
        "axes[5].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Reporte de clasificación del mejor modelo\n",
        "best_model_results = results[best_model_name]\n",
        "print(f\"\\nREPORTE DE CLASIFICACIÓN - {best_model_name}\")\n",
        "print(\"=\" * 50)\n",
        "print(classification_report(y_test, best_model_results['y_test_pred'], \n",
        "                          target_names=['No Sobrevivió', 'Sobrevivió']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Análisis Detallado del Mejor Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análisis del mejor modelo\n",
        "best_model = best_models[best_model_name]\n",
        "best_results = results[best_model_name]\n",
        "\n",
        "print(f\"ANÁLISIS DETALLADO: {best_model_name}\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Mejores hiperparámetros:\")\n",
        "for param, value in best_results['best_params'].items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "print(f\"\\nMétricas de rendimiento:\")\n",
        "print(f\"  CV AUC Score: {best_results['cv_auc']:.4f}\")\n",
        "print(f\"  Test Accuracy: {best_results['test_accuracy']:.4f}\")\n",
        "print(f\"  Test Precision: {best_results['test_precision']:.4f}\")\n",
        "print(f\"  Test Recall: {best_results['test_recall']:.4f}\")\n",
        "print(f\"  Test F1-Score: {best_results['test_f1']:.4f}\")\n",
        "print(f\"  Test AUC: {best_results['test_auc']:.4f}\")\n",
        "\n",
        "# Feature importance (si está disponible)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': selected_features,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(f\"\\nImportancia de features:\")\n",
        "    for idx, row in feature_importance.head(10).iterrows():\n",
        "        print(f\"  {row['feature']:30s}: {row['importance']:.4f}\")\n",
        "    \n",
        "    # Visualizar feature importance\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(data=feature_importance.head(10), x='importance', y='feature', palette='viridis')\n",
        "    plt.title(f'Top 10 Feature Importances - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.ylabel('Features')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\nelif hasattr(best_model, 'coef_'):\n",
        "    # Para modelos lineales\n",
        "    feature_coef = pd.DataFrame({\n",
        "        'feature': selected_features,\n",
        "        'coefficient': best_model.coef_[0],\n",
        "        'abs_coefficient': np.abs(best_model.coef_[0])\n",
        "    }).sort_values('abs_coefficient', ascending=False)\n",
        "    \n",
        "    print(f\"\\nCoeficientes del modelo:\")\n",
        "    for idx, row in feature_coef.head(10).iterrows():\n",
        "        print(f\"  {row['feature']:30s}: {row['coefficient']:8.4f}\")\n",
        "    \n",
        "    # Visualizar coeficientes\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    colors = ['red' if x < 0 else 'blue' for x in feature_coef.head(10)['coefficient']]\n",
        "    plt.barh(range(10), feature_coef.head(10)['coefficient'], color=colors, alpha=0.7)\n",
        "    plt.yticks(range(10), feature_coef.head(10)['feature'])\n",
        "    plt.xlabel('Coefficient Value')\n",
        "    plt.title(f'Top 10 Feature Coefficients - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "    plt.axvline(x=0, color='black', linestyle='--', alpha=0.7)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Curva Precision-Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Curvas Precision-Recall para todos los modelos\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
        "\n",
        "for i, (model_name, model_results) in enumerate(results.items()):\n",
        "    # Calcular curva Precision-Recall\n",
        "    precision, recall, _ = precision_recall_curve(y_test, model_results['y_test_proba'])\n",
        "    pr_auc = auc(recall, precision)\n",
        "    \n",
        "    # Plotear curva\n",
        "    plt.plot(recall, precision, color=colors[i], lw=2, \n",
        "             label=f'{model_name} (PR-AUC = {pr_auc:.3f})')\n",
        "\n",
        "# Línea baseline (proporción de clase positiva)\n",
        "baseline = y_test.sum() / len(y_test)\n",
        "plt.axhline(y=baseline, color='gray', lw=2, linestyle='--', alpha=0.7, \n",
        "           label=f'Baseline (P = {baseline:.3f})')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall (Sensitivity)', fontsize=12)\n",
        "plt.ylabel('Precision', fontsize=12)\n",
        "plt.title('Curvas Precision-Recall - Comparación de Modelos', fontsize=16, fontweight='bold')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Análisis de umbrales para el mejor modelo\n",
        "print(f\"\\nANÁLISIS DE UMBRALES - {best_model_name}\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Generar diferentes umbrales\n",
        "thresholds = np.arange(0.1, 1.0, 0.1)\n",
        "threshold_results = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_thresh = (best_results['y_test_proba'] >= threshold).astype(int)\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred_thresh)\n",
        "    precision = precision_score(y_test, y_pred_thresh)\n",
        "    recall = recall_score(y_test, y_pred_thresh)\n",
        "    f1 = f1_score(y_test, y_pred_thresh)\n",
        "    \n",
        "    threshold_results.append({\n",
        "        'Threshold': threshold,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1_Score': f1\n",
        "    })\n",
        "\n",
        "threshold_df = pd.DataFrame(threshold_results)\n",
        "display(threshold_df.round(3))\n",
        "\n",
        "# Encontrar mejor umbral basado en F1-Score\n",
        "best_threshold_idx = threshold_df['F1_Score'].idxmax()\n",
        "best_threshold = threshold_df.iloc[best_threshold_idx]['Threshold']\n",
        "best_f1 = threshold_df.iloc[best_threshold_idx]['F1_Score']\n",
        "\n",
        "print(f\"\\nMejor umbral basado en F1-Score: {best_threshold:.1f}\")\n",
        "print(f\"F1-Score con mejor umbral: {best_f1:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Validación Cruzada Detallada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validación cruzada detallada para el mejor modelo\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "print(f\"VALIDACIÓN CRUZADA DETALLADA - {best_model_name}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Métricas múltiples\n",
        "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
        "\n",
        "# Seleccionar datos apropiados según el modelo\n",
        "if best_model_name in ['Logistic Regression', 'Support Vector Classifier']:\n",
        "    X_cv = X_train_scaled\nelse:\n",
        "    X_cv = X_train\n",
        "\n",
        "cv_results = cross_validate(\n",
        "    best_model, X_cv, y_train, \n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), \n",
        "    scoring=scoring, \n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Procesar resultados\n",
        "metrics_summary = {}\n",
        "for metric in scoring:\n",
        "    train_key = f'train_{metric}'\n",
        "    test_key = f'test_{metric}'\n",
        "    \n",
        "    metrics_summary[metric.upper()] = {\n",
        "        'train': cv_results[train_key],\n",
        "        'test': cv_results[test_key]\n",
        "    }\n",
        "\n",
        "# Mostrar estadísticas\n",
        "for metric_name, metric_data in metrics_summary.items():\n",
        "    train_scores = metric_data['train']\n",
        "    test_scores = metric_data['test']\n",
        "    \n",
        "    print(f\"\\n{metric_name}:\")\n",
        "    print(f\"  Train: {train_scores.mean():.4f} ± {train_scores.std():.4f}\")\n",
        "    print(f\"  Test:  {test_scores.mean():.4f} ± {test_scores.std():.4f}\")\n",
        "\n",
        "# Visualización de validación cruzada\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, (metric_name, metric_data) in enumerate(metrics_summary.items()):\n",
        "    train_scores = metric_data['train']\n",
        "    test_scores = metric_data['test']\n",
        "    \n",
        "    x_pos = [1, 2]\n",
        "    means = [train_scores.mean(), test_scores.mean()]\n",
        "    stds = [train_scores.std(), test_scores.std()]\n",
        "    \n",
        "    axes[idx].bar(x_pos, means, yerr=stds, capsize=5, \n",
        "                  color=['skyblue', 'orange'], alpha=0.7)\n",
        "    axes[idx].set_xticks(x_pos)\n",
        "    axes[idx].set_xticklabels(['Train', 'Test'])\n",
        "    axes[idx].set_ylabel(metric_name)\n",
        "    axes[idx].set_title(f'{metric_name} - Validación Cruzada', fontweight='bold')\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Añadir valores a las barras\n",
        "    for i, (mean, std) in enumerate(zip(means, stds)):\n",
        "        axes[idx].text(x_pos[i], mean + std + 0.01, f'{mean:.3f}±{std:.3f}', \n",
        "                       ha='center', fontweight='bold')\n",
        "\n",
        "# Ocultar subplot extra\n",
        "axes[5].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Resumen y Conclusiones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"RESUMEN FINAL - MODELADO DE CLASIFICACIÓN\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "print(f\"Target Variable: {target}\")\n",
        "print(f\"Features utilizadas: {len(selected_features)}\")\n",
        "print(f\"Tamaño del dataset: {df.shape[0]:,} registros\")\n",
        "print(f\"Split train/test: {len(X_train)}/{len(X_test)} (80/20)\")\n",
        "print(f\"Balanceamiento del target: {balance_status}\")\n",
        "\n",
        "print(f\"\\nMODELOS EVALUADOS (ordenados por Test AUC):\")\n",
        "for idx, row in results_df.iterrows():\n",
        "    print(f\"{idx+1}. {row['Model']:25s}: AUC = {row['Test_AUC']:.4f}, F1 = {row['Test_F1']:.4f}\")\n",
        "\n",
        "print(f\"\\nMEJOR MODELO: {best_model_name}\")\n",
        "print(f\"Test AUC: {results_df.iloc[0]['Test_AUC']:.4f}\")\n",
        "print(f\"Test Accuracy: {results_df.iloc[0]['Test_Accuracy']:.4f}\")\n",
        "print(f\"Test F1-Score: {results_df.iloc[0]['Test_F1']:.4f}\")\n",
        "print(f\"Test Precision: {results_df.iloc[0]['Test_Precision']:.4f}\")\n",
        "print(f\"Test Recall: {results_df.iloc[0]['Test_Recall']:.4f}\")\n",
        "\n",
        "# Verificar si se cumple el objetivo de AUC > 0.8\n",
        "auc_objective_met = results_df.iloc[0]['Test_AUC'] > 0.8\n",
        "print(f\"\\nOBJETIVO AUC > 0.8: {'CUMPLIDO' if auc_objective_met else 'NO CUMPLIDO'}\")\n",
        "\n",
        "# Verificar métricas adicionales\n",
        "f1_good = results_df.iloc[0]['Test_F1'] > 0.7\n",
        "precision_good = results_df.iloc[0]['Test_Precision'] > 0.7\n",
        "recall_good = results_df.iloc[0]['Test_Recall'] > 0.7\n",
        "\n",
        "print(f\"F1-Score > 0.7: {'CUMPLIDO' if f1_good else 'NO CUMPLIDO'}\")\n",
        "print(f\"Precision > 0.7: {'CUMPLIDO' if precision_good else 'NO CUMPLIDO'}\")\n",
        "print(f\"Recall > 0.7: {'CUMPLIDO' if recall_good else 'NO CUMPLIDO'}\")\n",
        "\n",
        "if not auc_objective_met:\n",
        "    print(f\"\\nRECOMENDACIONES PARA MEJORAR:\")\n",
        "    print(f\"- Aplicar técnicas de balanceo de clases (SMOTE, undersampling)\")\n",
        "    print(f\"- Incluir más features relevantes\")\n",
        "    print(f\"- Probar ensemble methods\")\n",
        "    print(f\"- Ajustar umbrales de clasificación\")\n",
        "    print(f\"- Revisar feature engineering\")\n",
        "\n",
        "# Análisis de overfitting\n",
        "train_test_gap = results_df.iloc[0]['Train_AUC'] - results_df.iloc[0]['Test_AUC']\n",
        "print(f\"\\nANÁLISIS DE OVERFITTING:\")\n",
        "print(f\"Gap Train-Test AUC: {train_test_gap:.4f}\")\n",
        "if train_test_gap > 0.1:\n",
        "    print(f\"ADVERTENCIA: Posible overfitting detectado\")\n",
        "elif train_test_gap < 0.05:\n",
        "    print(f\"EXCELENTE: Buen balance entre bias y variance\")\n",
        "else:\n",
        "    print(f\"BUENO: Gap aceptable\")\n",
        "\n",
        "print(f\"\\nFEATURES MÁS IMPORTANTES:\")\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': selected_features,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    for idx, row in feature_importance.head(5).iterrows():\n",
        "        print(f\"{idx+1}. {row['feature']:30s}: {row['importance']:.4f}\")\nelif hasattr(best_model, 'coef_'):\n",
        "    feature_coef = pd.DataFrame({\n",
        "        'feature': selected_features,\n",
        "        'abs_coefficient': np.abs(best_model.coef_[0])\n",
        "    }).sort_values('abs_coefficient', ascending=False)\n",
        "    \n",
        "    for idx, row in feature_coef.head(5).iterrows():\n",
        "        print(f\"{idx+1}. {row['feature']:30s}: {row['abs_coefficient']:.4f}\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 55)\n",
        "print(f\"MODELADO DE CLASIFICACIÓN COMPLETADO\")\n",
        "print(f\"=\" * 55)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}